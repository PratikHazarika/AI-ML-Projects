{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Intent Classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNBY3oJGISe9X9UCuJD1mFi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PratikHazarika/AI-ML-Projects/blob/main/NLP/Intent%20Classification/src/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading packages"
      ],
      "metadata": {
        "id": "Cqj1dL4R6zmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pickle-mixin.\n",
        "! pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1CqIPsM6vmR",
        "outputId": "c2b4cd9a-e4c2-492d-e40b-92a492cf081c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Invalid requirement: 'pickle-mixin.'\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing required libraries"
      ],
      "metadata": {
        "id": "__ib37rj6F5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import wget, json, keras, pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pprint import pprint as pp\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Embedding,\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "lFPD8x39U2-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df6bc022-7e75-4f57-8904-97f2d750e490"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "KoEfQqkNVGiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading CLINC 150 dataset"
      ],
      "metadata": {
        "id": "DAbZfRb5VtMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BbB_QK7B6RYa"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/clinc/oos-eval/master/data/data_full.json'\n",
        "wget.download(url)\n",
        "\n",
        "glove ='https://www.dropbox.com/s/a247ju2qsczh0be/glove.6B.100d.txt?dl=1'\n",
        "wget.download(glove)\n",
        "\n",
        "\n",
        "with open('data_full.json') as file:\n",
        "  sample_data = json.loads(file.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the dataset"
      ],
      "metadata": {
        "id": "SPZUykpgb9X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data in sample_data:\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "RI7h0S5neiYv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa01ee64-3b32-4b93-a692-11247acc4b20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "oos_val\n",
            "val\n",
            "train\n",
            "oos_test\n",
            "test\n",
            "oos_train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Sizes"
      ],
      "metadata": {
        "id": "mWen_n_mkmc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"test size: {len(sample_data['test'])}\")\n",
        "print(f\"train size: {len(sample_data['train'])}\")\n",
        "print(f\"oos train size: {len(sample_data['oos_train'])}\")\n",
        "print(f\"oos train size: {len(sample_data['oos_test'])}\")\n",
        "print(f\"val: {len(sample_data['val'])}\")"
      ],
      "metadata": {
        "id": "xHtikzNZko-6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baece7c7-932c-43a7-a72d-e09962536a1e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test size: 4500\n",
            "train size: 15000\n",
            "oos train size: 100\n",
            "oos train size: 1000\n",
            "val: 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intents "
      ],
      "metadata": {
        "id": "A4IvXcWRcWaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intents = []\n",
        "\n",
        "for data in sample_data['train']:\n",
        "  intents.append(data[1])\n",
        "\n",
        "print(f'Total intents in the dataset = {len(set(intents))} \\n')"
      ],
      "metadata": {
        "id": "92SiMadOcARW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac60ccd5-1d0b-436f-a105-96f919dd3b0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total intents in the dataset = 150 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for intent in set(intents):\n",
        "  print(intent)"
      ],
      "metadata": {
        "id": "KNbDZnlb8EWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train sentences"
      ],
      "metadata": {
        "id": "VSRWRL_lkCf_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for train_sentence in sample_data['train']:\n",
        "  print(train_sentence[0])"
      ],
      "metadata": {
        "id": "ZHLnldF4kGAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test sentences"
      ],
      "metadata": {
        "id": "N7y7rB_OkVVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for test_sentence in sample_data['train']:\n",
        "  print(test_sentence[0])"
      ],
      "metadata": {
        "id": "y-q8-JqDkXI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OOS data"
      ],
      "metadata": {
        "id": "-9k0nz4mdl6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oos_train = []\n",
        "\n",
        "for data in sample_data['oos_train']:\n",
        "  oos_train.append(data)\n",
        "\n",
        "print(f'number of oos train values = {len((oos_train))}')\n",
        "\n",
        "for data in oos_train:\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "4hVx1EuDcdt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oos_test = []\n",
        "\n",
        "for data in sample_data['oos_test']:\n",
        "  oos_test.append(data)\n",
        "\n",
        "print(f'number of oos test values = {len((oos_test))}')\n",
        "\n",
        "for data in oos_test:\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "C334v8B2iDQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Val data"
      ],
      "metadata": {
        "id": "vYFuDrgf8de3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for data in sample_data['val']:\n",
        "  print(data)"
      ],
      "metadata": {
        "id": "QMI27Cg4fQd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading out of scope and other data\n",
        "Most supervised machine learning tasks assume a dataset with a set of well-defined target intent set. But what happens when a trained model meets the real world, where inputs to the trained model might not be from the well-defined target intent set? This dataset offers a way to evaluate intent classification models on \"out-of-scope\" inputs.\n",
        "\n",
        "\"Out-of-scope\" inputs are those that do not belong to the set of \"in-scope\" target intents. You may have heard other ways of referring to out-of-scope, including \"out-of-domain\" or \"out-of-distribution\"."
      ],
      "metadata": {
        "id": "QyYw8DOBVnrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = sample_data\n",
        "\n",
        "val_oos = np.array(data['oos_val'])\n",
        "train_oos = np.array(data['oos_train'])\n",
        "test_oos = np.array(data['oos_test'])\n",
        "\n",
        "val_others = np.array(data['val'])\n",
        "train_others = np.array(data['train'])\n",
        "test_others = np.array(data['test'])"
      ],
      "metadata": {
        "id": "bXS85vVs6YqH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging oos and other data"
      ],
      "metadata": {
        "id": "iwFsUG0fWCGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val = np.concatenate([val_oos, val_others])\n",
        "train = np.concatenate([train_oos, train_others])\n",
        "test = np.concatenate([test_oos, test_others])\n",
        "\n",
        "data = np.concatenate([train, test, val])\n",
        "data = data.T"
      ],
      "metadata": {
        "id": "qCa_7wED6tPx"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Splitting training and test data"
      ],
      "metadata": {
        "id": "SIh4hy4yWL9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = data[0] \n",
        "intents = data[1] \n",
        "train_txt, test_txt, train_intent, test_intents = train_test_split(text, intents, test_size = 0.3) #saving the train and test data"
      ],
      "metadata": {
        "id": "g5TChT8k6zBz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data processing\n",
        "## Tokenizing and converting words to integers"
      ],
      "metadata": {
        "id": "Mg5tv_yHWTxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_num_words = 4000\n",
        "classes = np.unique(intents) #getting the unique intents\n",
        "\n",
        "tokenizer = Tokenizer(num_words = max_num_words) # tokenizing the sentences\n",
        "tokenizer.fit_on_texts(train_txt)                # create a dict and counts # occurences of each word\n",
        "word_index = tokenizer.word_index                # contains a dict with unique words and their counts "
      ],
      "metadata": {
        "id": "zm5LShDu62PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Padding the phrases"
      ],
      "metadata": {
        "id": "bAOJe6UYWc1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ls = [len(sentence.split()) for sentence in train_txt] # creating a list of the sentence lengths    \n",
        "max_len = int(np.percentile(ls, 98))                   # max len of sentence is 98th percentile of the len\n",
        "\n",
        "train_sequences = tokenizer.texts_to_sequences(train_txt)                          # converting train text tokens of text into integers\n",
        "train_sequences = pad_sequences(train_sequences, maxlen = max_len, padding='post') # padding the sequences (adding 0 when needed)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_txt)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen = max_len, padding='post')"
      ],
      "metadata": {
        "id": "dVbncWWp658P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding"
      ],
      "metadata": {
        "id": "6PH0eArXWjw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "intent_encoder = LabelEncoder() \n",
        "integer_encoded = intent_encoder.fit_transform(classes)\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
        "onehot_encoder.fit(integer_encoded)\n",
        "\n",
        "train_intent_encoded = intent_encoder.transform(train_intent) \n",
        "train_intent_encoded = train_intent_encoded.reshape(len(train_intent_encoded), 1)\n",
        "train_intent = onehot_encoder.transform(train_intent_encoded)\n",
        "\n",
        "test_intents_encoded = intent_encoder.transform(test_intents)\n",
        "test_intents_encoded = test_intents_encoded.reshape(len(test_intents_encoded), 1)\n",
        "test_intents = onehot_encoder.transform(test_intents_encoded)"
      ],
      "metadata": {
        "id": "vcUSJ2AT676j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embedding\n",
        "### Preparing GloVe"
      ],
      "metadata": {
        "id": "avDBVF-mWov6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index={} #dict to store the trained words\n",
        "\n",
        "with open('glove.6B.100d.txt', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs"
      ],
      "metadata": {
        "id": "P1Iyzeka6-un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe embedding"
      ],
      "metadata": {
        "id": "8tsShRXzWx6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_embs = np.stack(embeddings_index.values())\n",
        "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
        "num_words = min(max_num_words, len(word_index)) + 1\n",
        "embedding_dim = len(embeddings_index['the'])\n",
        "embedding_matrix = np.random.normal(emb_mean, emb_std, (num_words, embedding_dim))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "    if i >= max_num_words:\n",
        "        break\n",
        "        \n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "LZJbEMWdWvws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "## Making Model"
      ],
      "metadata": {
        "id": "t500VVuAW6B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(num_words, 100, trainable=False, input_length=train_sequences.shape[1], weights=[embedding_matrix]))\n",
        "model.add(Bidirectional(LSTM(256, return_sequences=True, recurrent_dropout=0.1, dropout=0.1), 'concat'))\n",
        "model.add(Dropout(0.3)) \n",
        "model.add(LSTM(256, return_sequences=False, recurrent_dropout=0.1, dropout=0.1))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(classes.shape[0], activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
      ],
      "metadata": {
        "id": "b81pXCH4W-T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Model"
      ],
      "metadata": {
        "id": "SQp2irDy7DRm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_sequences, train_intent, epochs = 1,\n",
        "          batch_size = 128, shuffle=True,\n",
        "          validation_data=[test_sequences, test_intents])"
      ],
      "metadata": {
        "id": "XdxwxRzw7Gjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving Model"
      ],
      "metadata": {
        "id": "MSOIYJRgXLe-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('drive/MyDrive/Colab Notebooks/models/intents.h5') \n",
        "\n",
        "with open('drive/MyDrive/Colab Notebooks/utils/classes.pkl','wb') as file:\n",
        "   pickle.dump(classes,file)\n",
        "\n",
        "with open('drive/MyDrive/Colab Notebooks/utils/tokenizer.pkl','wb') as file:\n",
        "   pickle.dump(tokenizer,file)\n",
        "\n",
        "with open('drive/MyDrive/Colab Notebooks/utils/label_encoder.pkl','wb') as file:\n",
        "   pickle.dump(label_encoder,file)"
      ],
      "metadata": {
        "id": "wZFSuX8OHCsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "qN3XdVhfXUTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('gdrive/MyDrive/Colab Notebooks/models/intents.h5')\n",
        "\n",
        "with open('gdrive/MyDrive/Colab Notebooks/utils/classes.pkl','rb') as file:\n",
        "  classes = pickle.load(file)\n",
        "\n",
        "with open('gdrive/MyDrive/Colab Notebooks/utils/tokenizer.pkl','rb') as file:\n",
        "  tokenizer = pickle.load(file)\n",
        "\n",
        "with open('gdrive/MyDrive/Colab Notebooks/utils/label_encoder.pkl','rb') as file:\n",
        "  label_encoder = pickle.load(file)"
      ],
      "metadata": {
        "id": "pC6LkLk0NS5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "2ENtizheNUFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IntentClassifier:\n",
        "    def __init__(self,classes,model,tokenizer,label_encoder):\n",
        "        self.classes = classes\n",
        "        self.classifier = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.label_encoder = label_encoder\n",
        "\n",
        "    def get_intent(self,text):\n",
        "        self.text = [text] # input text\n",
        "        self.test_keras = self.tokenizer.texts_to_sequences(self.text) # converted text to int\n",
        "        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post') # padding the text sequence\n",
        "        self.pred = self.classifier.predict(self.test_keras_sequence) # predicting the intent using padded sequence\n",
        "\n",
        "        return self.label_encoder.inverse_transform(np.argmax(self.pred, axis=1))[0] \n",
        "\n",
        "    def get_probability(self, text):\n",
        "        self.text = [text] # input text\n",
        "        self.test_keras = self.tokenizer.texts_to_sequences(self.text) # converted text to int\n",
        "        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post') # padding the text sequence\n",
        "        self.pred = self.classifier.predict(self.test_keras_sequence) # predicting the intent using padded sequence\n",
        "\n",
        "        sorted = np.sort(self.pred[0])\n",
        "        \n",
        "        return [sorted[-1], sorted[-2], sorted[-2]]\n",
        "\n",
        "    def get_intents(self, text):\n",
        "        self.text = [text] # input text\n",
        "        self.test_keras = self.tokenizer.texts_to_sequences(self.text) # converted text to int\n",
        "        self.test_keras_sequence = pad_sequences(self.test_keras, maxlen=16, padding='post') # padding the text sequence\n",
        "        self.pred = self.classifier.predict(self.test_keras_sequence) # predicting the intent using padded sequence\n",
        "\n",
        "        sorted = np.sort(self.pred[0])\n",
        "\n",
        "        highest_index = np.where(self.pred[0] == sorted[-1])\n",
        "        second_highest_index = np.where(self.pred[0] == sorted[-2])\n",
        "        third_highest_index = np.where(self.pred[0] == sorted[-3])\n",
        "\n",
        "        return [self.label_encoder.inverse_transform(highest_index)[0], self.label_encoder.inverse_transform(second_highest_index)[0], self.label_encoder.inverse_transform(third_highest_index)[0]]\n",
        "\n"
      ],
      "metadata": {
        "id": "IeDobiphNiPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlu = IntentClassifier(classes,model,tokenizer,label_encoder)\n",
        "val = sample_data['val']\n",
        "sent = val[0][0]\n",
        "intent = val[0][1]\n",
        "threshold = 0.64\n",
        "\n",
        "for sentence in val[:500]:\n",
        "  intent = sentence[1]\n",
        "  sent = sentence[0]\n",
        "\n",
        "  if nlu.get_probability(sent)[0] > threshold:\n",
        "    print(f\"Sentence: {sent}\")\n",
        "    print(f\"Intent: {intent}\\n\\n\")\n",
        "\n",
        "  else:\n",
        "    intent = \"out of scope\"\n",
        "    print(f\"Sentence: {sent}\")\n",
        "    print(f\"Intent: {intent}\\n\\n\")  "
      ],
      "metadata": {
        "id": "w99vWuXcX9EF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}